{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68f8dd51-e300-4dc5-99a4-36e3fcd2c51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in links: /usr/share/pip-wheels\n",
      "Collecting bs4\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/envs/anaconda-2024.02-py310/lib/python3.10/site-packages (from bs4) (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/envs/anaconda-2024.02-py310/lib/python3.10/site-packages (from beautifulsoup4->bs4) (2.5)\n",
      "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Installing collected packages: bs4\n",
      "Successfully installed bs4-0.0.2\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in links: /usr/share/pip-wheels\n",
      "Requirement already satisfied: html5lib in ./.local/lib/python3.10/site-packages (1.1)\n",
      "Requirement already satisfied: pandas in /opt/conda/envs/anaconda-2024.02-py310/lib/python3.10/site-packages (2.1.4)\n",
      "Requirement already satisfied: requests in ./.local/lib/python3.10/site-packages (2.32.3)\n",
      "Requirement already satisfied: six>=1.9 in /opt/conda/envs/anaconda-2024.02-py310/lib/python3.10/site-packages (from html5lib) (1.16.0)\n",
      "Requirement already satisfied: webencodings in /opt/conda/envs/anaconda-2024.02-py310/lib/python3.10/site-packages (from html5lib) (0.5.1)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /opt/conda/envs/anaconda-2024.02-py310/lib/python3.10/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/envs/anaconda-2024.02-py310/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/envs/anaconda-2024.02-py310/lib/python3.10/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/envs/anaconda-2024.02-py310/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/anaconda-2024.02-py310/lib/python3.10/site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/anaconda-2024.02-py310/lib/python3.10/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/anaconda-2024.02-py310/lib/python3.10/site-packages (from requests) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/anaconda-2024.02-py310/lib/python3.10/site-packages (from requests) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "#basics of the BeautifulSoup Python library\n",
    "#we are going to be using Python and several Python libraries.\n",
    "!pip install bs4\n",
    "!pip install html5lib pandas requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2dc1e115-4526-400a-ad33-308344843ec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n<!DOCTYPE html>\\n<html>\\n<head>\\n<title>Page Title</title>\\n</head>\\n<body>\\n<h3><b id='boldest'>Lebron James</b></h3>\\n<p> Salary: $ 92,000,000 </p>\\n<h3> Stephen Curry</h3>\\n<p> Salary: $85,000, 000 </p>\\n<h3> Kevin Durant </h3>\\n<p> Salary: $73,200, 000</p>\\n</body>\\n</html>\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup # this module helps in web scrapping.\n",
    "import requests # this module helps us to download a web page\n",
    "\n",
    "'''\n",
    "Beautiful Soup is a Python library for pulling data out of HTML and XML files, \n",
    "we will focus on HTML files. This is accomplished by representing the HTML as a \n",
    "set of objects with methods used to parse the HTML. We can navigate the HTML as a tree, \n",
    "and/or filter out what we are looking for.\n",
    "'''\n",
    "\n",
    "#HTML\n",
    "\n",
    "'''\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "<title>Page Title</title>\n",
    "</head>\n",
    "<body>\n",
    "<h3><b id='boldest'>Lebron James</b></h3>\n",
    "<p> Salary: $ 92,000,000 </p>\n",
    "<h3> Stephen Curry</h3>\n",
    "<p> Salary: $85,000, 000 </p>\n",
    "<h3> Kevin Durant </h3>\n",
    "<p> Salary: $73,200, 000</p>\n",
    "</body>\n",
    "</html>\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "072539bd-9164-4d7a-9c4c-d8867d4880ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#store above given HTML as a string in the variable html\n",
    "html=\"<!DOCTYPE html><html><head><title>Page Title</title></head><body><h3><b id='boldest'>Lebron James</b></h3><p> Salary: $ 92,000,000 </p><h3> Stephen Curry</h3><p> Salary: $85,000, 000 </p><h3> Kevin Durant </h3><p> Salary: $73,200, 000</p></body></html>\"\n",
    "'''\n",
    "To parse a document, pass it into the BeautifulSoup constructor. \n",
    "The BeautifulSoup object represents the document as a nested data structure:\n",
    "'''\n",
    "soup=BeautifulSoup(html,'html5lib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e081adf3-d349-4c48-aa37-866495639154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html>\n",
      " <head>\n",
      "  <title>\n",
      "   Page Title\n",
      "  </title>\n",
      " </head>\n",
      " <body>\n",
      "  <h3>\n",
      "   <b id=\"boldest\">\n",
      "    Lebron James\n",
      "   </b>\n",
      "  </h3>\n",
      "  <p>\n",
      "   Salary: $ 92,000,000\n",
      "  </p>\n",
      "  <h3>\n",
      "   Stephen Curry\n",
      "  </h3>\n",
      "  <p>\n",
      "   Salary: $85,000, 000\n",
      "  </p>\n",
      "  <h3>\n",
      "   Kevin Durant\n",
      "  </h3>\n",
      "  <p>\n",
      "   Salary: $73,200, 000\n",
      "  </p>\n",
      " </body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''First, the document is converted to Unicode (similar to ASCII) and HTML entities are converted to Unicode characters. \n",
    "Beautiful Soup transforms a complex HTML document into a complex tree of Python objects. \n",
    "The BeautifulSoup object can create other types of objects.'''\n",
    "\n",
    "#We can use the method prettify() to display the HTML in the nested structure:\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "17bbfff6-1fa9-44bc-9fa9-aced90fc1ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title obtained through title tag: <title>Page Title</title>\n",
      "type of title tag <class 'bs4.element.Tag'>\n"
     ]
    }
   ],
   "source": [
    "#Let's say we want the title of the page and the name of the top paid player. We can use the Tag.\n",
    "tag_object=soup.title\n",
    "print(\"Title obtained through title tag:\", tag_object)\n",
    "\n",
    "#see the tag type\n",
    "print(\"type of title tag\",type(tag_object))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2cf90cf4-c19d-4450-aeba-70cf223632e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h3><b id=\"boldest\">Lebron James</b></h3>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#If there is more than one Tag with the same name, the first element with that Tag name is called.\n",
    "tag_object=soup.h3\n",
    "tag_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "16498b1d-af8a-42b1-9db2-8b7a75b42676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<b id=\"boldest\">Lebron James</b>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Children, Parents, and Siblings\n",
    "#As stated above, the Tag object is a tree of objects. We can access the child of the tag or navigate down the branch as follows:\n",
    "child_tag=tag_object.b\n",
    "child_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dfcf5882-2ad6-4297-9f7c-5cce9ed2beca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h3><b id=\"boldest\">Lebron James</b></h3>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#You can access the parent with the  \"parent\"\n",
    "parent_tag=child_tag.parent\n",
    "parent_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4c37732e-91df-40f3-9ffc-ef2bf2c2bbe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<body><h3><b id=\"boldest\">Lebron James</b></h3><p> Salary: $ 92,000,000 </p><h3> Stephen Curry</h3><p> Salary: $85,000, 000 </p><h3> Kevin Durant </h3><p> Salary: $73,200, 000</p></body>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#parent_tag output is identical to tag_object\n",
    "parent_tag.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f6af4290-4cc5-4f4e-9d32-042e992fcd1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<body><h3><b id=\"boldest\">Lebron James</b></h3><p> Salary: $ 92,000,000 </p><h3> Stephen Curry</h3><p> Salary: $85,000, 000 </p><h3> Kevin Durant </h3><p> Salary: $73,200, 000</p></body>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_object.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6989e0b2-91a1-47e9-9aef-9c4782c86b34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<p> Salary: $ 92,000,000 </p>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tag_object sibling is the paragraph element\n",
    "sibling_to_h3_1=tag_object.next_sibling\n",
    "sibling_to_h3_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3d1660e0-840f-49bd-85a2-184916b3b987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h3> Stephen Curry</h3>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sibling_to_h3_2 is the header element, which is also a sibling of both sibling_to_h3_1 and tag_object\n",
    "sibling_to_h3_2=sibling_to_h3_1.next_sibling\n",
    "sibling_to_h3_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "17c3ddaf-5f70-42ed-b499-74a64e402cf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<p> Salary: $85,000, 000 </p>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#similarly other paragraph element is also sibling of sibling_to_h3_1,sibling_to_h3_2 and tag_object\n",
    "sibling_to_h3_2.next_sibling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c1a8a858-8817-401a-a4d7-728f13f92d04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'boldest'}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#HTML Attributes\n",
    "'''If the tag has attributes, the tag id=\"boldest\" has an attribute id whose value is boldest. You can access a tag’s attributes by treating the tag like a dictionary:'''\n",
    "child_tag['id']\n",
    "\n",
    "#You can access that dictionary directly as attrs:\n",
    "child_tag.attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e1f2e19e-9747-4124-ae07-84f155ec01a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FOR FUTURE BEAUTIFULSOUP INFORMATION REFER BELOW GIVEN LINK, UNCOMMENT THE LINK AND SEARCH IT IN BROWSER\n",
    "#https://www.crummy.com/software/BeautifulSoup/bs4/doc/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8bf4a349-3adb-479d-9580-60861f908ad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'boldest'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We can also obtain the content of the attribute of the tag using the Python get() method.\n",
    "child_tag.get('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "eb3f06f7-5157-4dc2-aeb9-2b780af36fc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lebron James'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Navigable String\n",
    "#A string corresponds to a bit of text or content within a tag. Beautiful Soup uses the NavigableString class to contain this text.\n",
    "tag_string=child_tag.string\n",
    "tag_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7c709a16-ed23-4d4d-8a8c-28542682307c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.element.NavigableString"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we can verify the type is Navigable String\n",
    "type(tag_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "86fc3d01-7753-4dd1-8a0d-4a0b2022d8fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lebron James'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''A NavigableString is similar to a Python string or Unicode string. To be more precise, \n",
    "the main difference is that it also supports some BeautifulSoup features. We can convert it to string object in Python:'''\n",
    "unicode_string=str(tag_string)\n",
    "unicode_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c0eae58e-ea50-40ea-9df4-7e827e6a9fb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<table><tr><td id=\"flight\">Flight No</td><td>Launch site</td> <td>Payload mass</td></tr><tr> <td>1</td><td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida<a></a></a></td><td>300 kg</td></tr><tr><td>2</td><td><a href=\"https://en.wikipedia.org/wiki/Texas\">Texas</a></td><td>94 kg</td></tr><tr><td>3</td><td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida<a> </a></a></td><td>80 kg</td></tr></table>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#FILTER\n",
    "\n",
    "'''Filters allow you to find complex patterns, the simplest filter is a string. \n",
    "In this section we will pass a string to a different filter method and Beautiful Soup will perform a match against that \n",
    "exact string. Consider the following HTML of rocket launches:'''\n",
    "\n",
    "'''<table>\n",
    "  <tr>\n",
    "    <td id='flight' >Flight No</td>\n",
    "    <td>Launch site</td> \n",
    "    <td>Payload mass</td>\n",
    "   </tr>\n",
    "  <tr> \n",
    "    <td>1</td>\n",
    "    <td><a href='https://en.wikipedia.org/wiki/Florida'>Florida</a></td>\n",
    "    <td>300 kg</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>2</td>\n",
    "    <td><a href='https://en.wikipedia.org/wiki/Texas'>Texas</a></td>\n",
    "    <td>94 kg</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>3</td>\n",
    "    <td><a href='https://en.wikipedia.org/wiki/Florida'>Florida<a> </td>\n",
    "    <td>80 kg</td>\n",
    "  </tr>\n",
    "</table>'''\n",
    "\n",
    "table=\"<table><tr><td id='flight'>Flight No</td><td>Launch site</td> <td>Payload mass</td></tr><tr> <td>1</td><td><a href='https://en.wikipedia.org/wiki/Florida'>Florida<a></td><td>300 kg</td></tr><tr><td>2</td><td><a href='https://en.wikipedia.org/wiki/Texas'>Texas</a></td><td>94 kg</td></tr><tr><td>3</td><td><a href='https://en.wikipedia.org/wiki/Florida'>Florida<a> </td><td>80 kg</td></tr></table>\"\"\"\n",
    "table_soup=BeautifulSoup(table, 'html.parser')\n",
    "table_soup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1e5460eb-a764-4d64-b566-c82ae17527ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"https://en.wikipedia.org/wiki/Florida\">Florida<a></a></a>,\n",
       " <a href=\"https://en.wikipedia.org/wiki/Texas\">Texas</a>,\n",
       " <a href=\"https://en.wikipedia.org/wiki/Florida\">Florida<a> </a></a>]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''When we set the name parameter to a tag name, the method will extract all the tags with that name and its children'''\n",
    "\n",
    "table_href=table_soup.find_all('a', href=True)\n",
    "table_href"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ec0f9275-3b6c-455a-866a-acc789c46f1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tr><td id=\"flight\">Flight No</td><td>Launch site</td> <td>Payload mass</td></tr>,\n",
       " <tr> <td>1</td><td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida<a></a></a></td><td>300 kg</td></tr>,\n",
       " <tr><td>2</td><td><a href=\"https://en.wikipedia.org/wiki/Texas\">Texas</a></td><td>94 kg</td></tr>,\n",
       " <tr><td>3</td><td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida<a> </a></a></td><td>80 kg</td></tr>]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_rows=table_soup.find_all('tr')\n",
    "table_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "0fd3ae08-d4fe-4daf-9226-79336d11fbc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tr><td id=\"flight\">Flight No</td><td>Launch site</td> <td>Payload mass</td></tr>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_row=table_rows[0]\n",
    "first_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "f4797daa-5c2d-401e-a515-f6c37e181829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<td id=\"flight\">Flight No</td>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we can obtain the child \n",
    "\n",
    "first_row_child=first_row.td\n",
    "first_row_child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ef92c1e1-ff59-45b4-b918-ae7841a6b8eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tr><td id=\"flight\">Flight No</td><td>Launch site</td> <td>Payload mass</td></tr>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_row_child.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "801883fd-7fe1-49e1-b1bc-07f84daa0865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row 0 is <tr><td id=\"flight\">Flight No</td><td>Launch site</td> <td>Payload mass</td></tr>\n",
      "row 1 is <tr> <td>1</td><td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida<a></a></a></td><td>300 kg</td></tr>\n",
      "row 2 is <tr><td>2</td><td><a href=\"https://en.wikipedia.org/wiki/Texas\">Texas</a></td><td>94 kg</td></tr>\n",
      "row 3 is <tr><td>3</td><td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida<a> </a></a></td><td>80 kg</td></tr>\n"
     ]
    }
   ],
   "source": [
    "#If we iterate through the list, each element corresponds to a row in the table:\n",
    "#i=index to each row \n",
    "#row=itrating through each row \n",
    "\n",
    "for i, row in enumerate(table_rows):\n",
    "    print(\"row\", i,\"is\", row)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "d2a365df-e65e-4591-86cf-61fbb6fb26c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row 0\n",
      "column: 0 content <td id=\"flight\">Flight No</td>\n",
      "column: 1 content <td>Launch site</td>\n",
      "column: 2 content <td>Payload mass</td>\n",
      "row 1\n",
      "column: 0 content <td>1</td>\n",
      "column: 1 content <td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida<a></a></a></td>\n",
      "column: 2 content <td>300 kg</td>\n",
      "row 2\n",
      "column: 0 content <td>2</td>\n",
      "column: 1 content <td><a href=\"https://en.wikipedia.org/wiki/Texas\">Texas</a></td>\n",
      "column: 2 content <td>94 kg</td>\n",
      "row 3\n",
      "column: 0 content <td>3</td>\n",
      "column: 1 content <td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida<a> </a></a></td>\n",
      "column: 2 content <td>80 kg</td>\n"
     ]
    }
   ],
   "source": [
    "#now iterate through each row's table date and print each row and its columns\n",
    "for i, row in enumerate(table_rows):\n",
    "    print(\"row\", i)\n",
    "    #find each rows table data and store it in cells vaiable\n",
    "    cells=row.find_all('td')\n",
    "    for j, cell in enumerate(cells):\n",
    "        print(\"column:\", j, \"content\", cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "517917c3-752d-45d0-ad0e-e47a0815b3bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tr><td id=\"flight\">Flight No</td><td>Launch site</td> <td>Payload mass</td></tr>,\n",
       " <td id=\"flight\">Flight No</td>,\n",
       " <td>Launch site</td>,\n",
       " <td>Payload mass</td>,\n",
       " <tr> <td>1</td><td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida<a></a></a></td><td>300 kg</td></tr>,\n",
       " <td>1</td>,\n",
       " <td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida<a></a></a></td>,\n",
       " <td>300 kg</td>,\n",
       " <tr><td>2</td><td><a href=\"https://en.wikipedia.org/wiki/Texas\">Texas</a></td><td>94 kg</td></tr>,\n",
       " <td>2</td>,\n",
       " <td><a href=\"https://en.wikipedia.org/wiki/Texas\">Texas</a></td>,\n",
       " <td>94 kg</td>,\n",
       " <tr><td>3</td><td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida<a> </a></a></td><td>80 kg</td></tr>,\n",
       " <td>3</td>,\n",
       " <td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida<a> </a></a></td>,\n",
       " <td>80 kg</td>]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#If we use a list we can match against any item in that list.\n",
    "table_list=table_soup.find_all(['tr','td'])\n",
    "table_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "92e4c84e-3049-45a3-97ab-5dbde3807f88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<td id=\"flight\">Flight No</td>"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ATTRIBUTES\n",
    "'''If the argument is not recognized it will be turned into a filter on the tag’s attributes. \n",
    "For example with the id argument, Beautiful Soup will filter against each tag’s id attribute. \n",
    "For example, the first td elements have a value of id of flight, therefore we can filter based on that id value.'''\n",
    "\n",
    "table_soup.find(id=\"flight\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "2d74a844-fe3c-44f1-b4aa-f248f6d918b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"https://en.wikipedia.org/wiki/Florida\">Florida<a></a></a>,\n",
       " <a href=\"https://en.wikipedia.org/wiki/Texas\">Texas</a>,\n",
       " <a href=\"https://en.wikipedia.org/wiki/Florida\">Florida<a> </a></a>]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_soup.find_all('a')\n",
    "#or\n",
    "table_soup.find_all(href=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "ec10250c-f85d-4d8e-aa4b-b8bd3c6d0b6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"https://en.wikipedia.org/wiki/Florida\">Florida<a></a></a>,\n",
       " <a href=\"https://en.wikipedia.org/wiki/Florida\">Florida<a> </a></a>]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We can find all the elements that have links to the Florida Wikipedia page:\n",
    "table_soup.find_all(href=\"https://en.wikipedia.org/wiki/Florida\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "b849a6d2-d163-437a-a501-ad699466a365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<table><tr><td id=\"flight\">Flight No</td><td>Launch site</td> <td>Payload mass</td></tr><tr> <td>1</td><td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida<a></a></a></td><td>300 kg</td></tr><tr><td>2</td><td><a href=\"https://en.wikipedia.org/wiki/Texas\">Texas</a></td><td>94 kg</td></tr><tr><td>3</td><td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida<a> </a></a></td><td>80 kg</td></tr></table>,\n",
       " <tr><td id=\"flight\">Flight No</td><td>Launch site</td> <td>Payload mass</td></tr>,\n",
       " <td id=\"flight\">Flight No</td>,\n",
       " <td>Launch site</td>,\n",
       " <td>Payload mass</td>,\n",
       " <tr> <td>1</td><td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida<a></a></a></td><td>300 kg</td></tr>,\n",
       " <td>1</td>,\n",
       " <td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida<a></a></a></td>,\n",
       " <a></a>,\n",
       " <td>300 kg</td>,\n",
       " <tr><td>2</td><td><a href=\"https://en.wikipedia.org/wiki/Texas\">Texas</a></td><td>94 kg</td></tr>,\n",
       " <td>2</td>,\n",
       " <td><a href=\"https://en.wikipedia.org/wiki/Texas\">Texas</a></td>,\n",
       " <td>94 kg</td>,\n",
       " <tr><td>3</td><td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida<a> </a></a></td><td>80 kg</td></tr>,\n",
       " <td>3</td>,\n",
       " <td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida<a> </a></a></td>,\n",
       " <a> </a>,\n",
       " <td>80 kg</td>]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using the logic above, find all the elements without href value\n",
    "table_soup.find_all(href=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "949a0ade-8028-4fbb-bfb3-ee2e825c355c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Florida', 'Florida']"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#STRING\n",
    "#With string you can search for strings instead of tags, where we find all the elments with Florida:\n",
    "table_soup.find_all(string=\"Florida\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "e463e8a2-5613-47bf-ac71-d15dfd1ae85b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<h3>Rocket Launch </h3>\\n\\n<p>\\n<table class='rocket'>\\n  <tr>\\n    <td>Flight No</td>\\n    <td>Launch site</td> \\n    <td>Payload mass</td>\\n  </tr>\\n  <tr>\\n    <td>1</td>\\n    <td>Florida</td>\\n    <td>300 kg</td>\\n  </tr>\\n  <tr>\\n    <td>2</td>\\n    <td>Texas</td>\\n    <td>94 kg</td>\\n  </tr>\\n  <tr>\\n    <td>3</td>\\n    <td>Florida </td>\\n    <td>80 kg</td>\\n  </tr>\\n</table>\\n</p>\\n<p>\\n\\n<h3>Pizza Party  </h3>\\n  \\n    \\n<table class='pizza'>\\n  <tr>\\n    <td>Pizza Place</td>\\n    <td>Orders</td> \\n    <td>Slices </td>\\n   </tr>\\n  <tr>\\n    <td>Domino's Pizza</td>\\n    <td>10</td>\\n    <td>100</td>\\n  </tr>\\n  <tr>\\n    <td>Little Caesars</td>\\n    <td>12</td>\\n    <td >144 </td>\\n  </tr>\\n  <tr>\\n    <td>Papa John's </td>\\n    <td>15 </td>\\n    <td>165</td>\\n  </tr>\\n\""
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#FIND\n",
    "\n",
    "'''The find_all() method scans the entire document looking for results.\n",
    "It’s useful if you are looking for one element, as you can use the find() method to find the first element in the document. \n",
    "Consider the following two tables:'''\n",
    "\n",
    "'''<h3>Rocket Launch </h3>\n",
    "\n",
    "<p>\n",
    "<table class='rocket'>\n",
    "  <tr>\n",
    "    <td>Flight No</td>\n",
    "    <td>Launch site</td> \n",
    "    <td>Payload mass</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1</td>\n",
    "    <td>Florida</td>\n",
    "    <td>300 kg</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>2</td>\n",
    "    <td>Texas</td>\n",
    "    <td>94 kg</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>3</td>\n",
    "    <td>Florida </td>\n",
    "    <td>80 kg</td>\n",
    "  </tr>\n",
    "</table>\n",
    "</p>\n",
    "<p>\n",
    "\n",
    "<h3>Pizza Party  </h3>\n",
    "  \n",
    "    \n",
    "<table class='pizza'>\n",
    "  <tr>\n",
    "    <td>Pizza Place</td>\n",
    "    <td>Orders</td> \n",
    "    <td>Slices </td>\n",
    "   </tr>\n",
    "  <tr>\n",
    "    <td>Domino's Pizza</td>\n",
    "    <td>10</td>\n",
    "    <td>100</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Little Caesars</td>\n",
    "    <td>12</td>\n",
    "    <td >144 </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Papa John's </td>\n",
    "    <td>15 </td>\n",
    "    <td>165</td>\n",
    "  </tr>\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "bbf3456f-c037-4db0-8614-d332b69536fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_tables=\"<h3>Rocket Launch </h3><p><table class='rocket'><tr><td>Flight No</td><td>Launch site</td> <td>Payload mass</td></tr><tr><td>1</td><td>Florida</td><td>300 kg</td></tr><tr><td>2</td><td>Texas</td><td>94 kg</td></tr><tr><td>3</td><td>Florida </td><td>80 kg</td></tr></table></p><p><h3>Pizza Party  </h3><table class='pizza'><tr><td>Pizza Place</td><td>Orders</td> <td>Slices </td></tr><tr><td>Domino's Pizza</td><td>10</td><td>100</td></tr><tr><td>Little Caesars</td><td>12</td><td >144 </td></tr><tr><td>Papa John's </td><td>15 </td><td>165</td></tr>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "205ed21e-2f82-43db-8730-ce9ae3cf85f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "two_tables_soup= BeautifulSoup(two_tables, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "4407c698-8a07-46fe-ba08-526c768a3cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<table class=\"rocket\"><tr><td>Flight No</td><td>Launch site</td> <td>Payload mass</td></tr><tr><td>1</td><td>Florida</td><td>300 kg</td></tr><tr><td>2</td><td>Texas</td><td>94 kg</td></tr><tr><td>3</td><td>Florida </td><td>80 kg</td></tr></table>"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find() method just calls first table, where as find_all() method calls all the tables in the document \n",
    "two_tables_soup.find('table')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "c7f51912-f89b-4c94-8e67-5114b53e3aa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<table class=\"rocket\"><tr><td>Flight No</td><td>Launch site</td> <td>Payload mass</td></tr><tr><td>1</td><td>Florida</td><td>300 kg</td></tr><tr><td>2</td><td>Texas</td><td>94 kg</td></tr><tr><td>3</td><td>Florida </td><td>80 kg</td></tr></table>,\n",
       " <table class=\"pizza\"><tr><td>Pizza Place</td><td>Orders</td> <td>Slices </td></tr><tr><td>Domino's Pizza</td><td>10</td><td>100</td></tr><tr><td>Little Caesars</td><td>12</td><td>144 </td></tr><tr><td>Papa John's </td><td>15 </td><td>165</td></tr></table>]"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_tables_soup.find_all(\"table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "1724127e-7925-41c5-8b8f-716efa9162fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<table class=\"pizza\"><tr><td>Pizza Place</td><td>Orders</td> <td>Slices </td></tr><tr><td>Domino's Pizza</td><td>10</td><td>100</td></tr><tr><td>Little Caesars</td><td>12</td><td>144 </td></tr><tr><td>Papa John's </td><td>15 </td><td>165</td></tr></table>"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We can filter on the class attribute to find the second table, but because class is a keyword in Python, \n",
    "#we add an underscore to differentiate them.\n",
    "\n",
    "#class_   is used to call the 2nd table by using find() method, since it call only first table, by filtering on class attribute to  find the 2nd table \n",
    "\n",
    "\n",
    "two_tables_soup.find(\"table\", class_=\"pizza\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "c94b8005-d21a-474f-85da-b7ebdab7c133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<HTML>\\n  <HEAD>\\n     Access Denied\\n  </HEAD>\\n<BODY>\\n\\n<h1>Access Denied</h1>\\n\\n<p>\\nAccess to arbitrary websites is not available from free accounts;\\nyou can only access sites that are on our\\n<a href=\"http://www.pythonanywhere.com/whitelist\">allowlist</a>.\\nIf you want to suggest something to add to our whitelist\\ndrop us a line at PythonAnywhere Support <nbusercare@pythonanywhere.com>.  It will have\\nto have an official public API.\\n</p>\\n\\n\\n<p>\\nAlternatively, you can sign up for a paid account at\\n<a href=\"http://www.pythonanywhere.com/account/\">http://www.pythonanywhere.com/account/</a>\\n</p>\\n<p>\\nIf you have already got a paid account and you\\'re still getting this messge,\\nyou may need to reload your web app (from the \"Web\" tab) or restart\\nyour consoles.  If that doesn\\'t help, drop us a line at PythonAnywhere Support <nbusercare@pythonanywhere.com>.\\n</p>\\n\\n</BODY>\\n'"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Downloading And Scraping The Contents Of A Web Page\n",
    "\n",
    "'''We use get to download the contents of the webpage in text format and store in a variable called r_data:'''\n",
    "\n",
    "url = \"http://www.ibm.com\"\n",
    "r_data=requests.get(url).text\n",
    "r_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "bc905c1a-c4d0-426c-97f4-e64061268206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<html><head>\n",
       "     </head><body>Access Denied\n",
       "  \n",
       "\n",
       "\n",
       "<h1>Access Denied</h1>\n",
       "\n",
       "<p>\n",
       "Access to arbitrary websites is not available from free accounts;\n",
       "you can only access sites that are on our\n",
       "<a href=\"http://www.pythonanywhere.com/whitelist\">allowlist</a>.\n",
       "If you want to suggest something to add to our whitelist\n",
       "drop us a line at PythonAnywhere Support <nbusercare@pythonanywhere.com>.  It will have\n",
       "to have an official public API.\n",
       "</nbusercare@pythonanywhere.com></p>\n",
       "\n",
       "\n",
       "<p>\n",
       "Alternatively, you can sign up for a paid account at\n",
       "<a href=\"http://www.pythonanywhere.com/account/\">http://www.pythonanywhere.com/account/</a>\n",
       "</p>\n",
       "<p>\n",
       "If you have already got a paid account and you're still getting this messge,\n",
       "you may need to reload your web app (from the \"Web\" tab) or restart\n",
       "your consoles.  If that doesn't help, drop us a line at PythonAnywhere Support <nbusercare@pythonanywhere.com>.\n",
       "</nbusercare@pythonanywhere.com></p>\n",
       "\n",
       "\n",
       "</body></html>"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We create a BeautifulSoup object using the BeautifulSoup constructor\n",
    "\n",
    "soup=BeautifulSoup(r_data,'html5lib')\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "91561daf-313c-4f02-850d-68cd63284c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Links: <a href=\"http://www.pythonanywhere.com/whitelist\">allowlist</a>\n",
      "Links: <a href=\"http://www.pythonanywhere.com/account/\">http://www.pythonanywhere.com/account/</a>\n"
     ]
    }
   ],
   "source": [
    "#Scrape all links\n",
    "\n",
    "for links in soup.find_all(href=True):\n",
    "    print(\"Links:\", links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "092232be-73c3-4476-b309-de202f1cd79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrape all images Tags\n",
    "\n",
    "for j in soup.find_all('img'):\n",
    "    print(\"images:\",j)\n",
    "    print(j.get('src'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c0ceee-0c29-4690-82c1-d809d27b7601",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94726e69-0992-4a6d-8672-6b5cfb1f9553",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-2024.02-py310",
   "language": "python",
   "name": "conda-env-anaconda-2024.02-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
